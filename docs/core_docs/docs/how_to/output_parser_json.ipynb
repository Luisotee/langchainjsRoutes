{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b1b316",
   "metadata": {},
   "source": [
    "# How to parse JSON output\n",
    "\n",
    "While some model providers support [built-in ways to return structured output](/docs/how_to/structured_output), not all do. We can use an output parser to help users to specify an arbitrary JSON schema via the prompt, query a model for outputs that conform to that schema, and finally parse that schema as JSON.\n",
    "\n",
    ":::{.callout-note}\n",
    "Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON.\n",
    ":::\n",
    "\n",
    "```{=mdx}\n",
    "import PrerequisiteLinks from \"@theme/PrerequisiteLinks\";\n",
    "\n",
    "<PrerequisiteLinks content={`\n",
    "- [Chat models](/docs/concepts/#chat-models)\n",
    "- [Output parsers](/docs/concepts/#output-parsers)\n",
    "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
    "- [Structured output](/docs/how_to/structured_output)\n",
    "- [Chaining runnables together](/docs/how_to/sequence/)\n",
    "`}/>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae909b7a",
   "metadata": {},
   "source": [
    "The [`JsonOutputParser`](https://api.js.langchain.com/classes/langchain_core_output_parsers.JsonOutputParser.html) is one built-in option for prompting for and then parsing JSON output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c667607",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccf45a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  setup: \u001b[32m\"Why couldn't the bicycle stand up by itself?\"\u001b[39m,\n",
       "  punchline: \u001b[32m\"Because it was two tired!\"\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { JsonOutputParser } from \"@langchain/core/output_parsers\"\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\"\n",
    "\n",
    "// Define your desired data structure.\n",
    "interface Joke {\n",
    "    setup: string\n",
    "    punchline: string\n",
    "}\n",
    "\n",
    "// And a query intented to prompt a language model to populate the data structure.\n",
    "const jokeQuery = \"Tell me a joke.\";\n",
    "const formatInstructions = \"Respond with a valid JSON object, containing two fields: 'setup' and 'punchline'.\"\n",
    "\n",
    "// Set up a parser + inject instructions into the prompt template.\n",
    "const parser = new JsonOutputParser<Joke>()\n",
    "\n",
    "const prompt = new PromptTemplate({\n",
    "    template: \"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    inputVariables: [\"query\"],\n",
    "    partialVariables: {\"format_instructions\": formatInstructions},\n",
    "})\n",
    "\n",
    "const chain = prompt.pipe(model).pipe(parser);\n",
    "\n",
    "await chain.invoke({ query: jokeQuery })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d801be",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "The `JsonOutputParser` also supports streaming partial chunks. This is useful when the model returns partial JSON output in multiple chunks. The parser will keep track of the partial chunks and return the final JSON output when the model finishes generating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0309256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{ setup: \"\" }\n",
      "{ setup: \"Why\" }\n",
      "{ setup: \"Why couldn\" }\n",
      "{ setup: \"Why couldn't\" }\n",
      "{ setup: \"Why couldn't the\" }\n",
      "{ setup: \"Why couldn't the bicycle\" }\n",
      "{ setup: \"Why couldn't the bicycle stand\" }\n",
      "{ setup: \"Why couldn't the bicycle stand up\" }\n",
      "{ setup: \"Why couldn't the bicycle stand up by\" }\n",
      "{ setup: \"Why couldn't the bicycle stand up by itself\" }\n",
      "{\n",
      "  setup: \"Why couldn't the bicycle stand up by itself?\",\n",
      "  punchline: \"\"\n",
      "}\n",
      "{\n",
      "  setup: \"Why couldn't the bicycle stand up by itself?\",\n",
      "  punchline: \"It\"\n",
      "}\n",
      "{\n",
      "  setup: \"Why couldn't the bicycle stand up by itself?\",\n",
      "  punchline: \"It was\"\n",
      "}\n",
      "{\n",
      "  setup: \"Why couldn't the bicycle stand up by itself?\",\n",
      "  punchline: \"It was two\"\n",
      "}\n",
      "{\n",
      "  setup: \"Why couldn't the bicycle stand up by itself?\",\n",
      "  punchline: \"It was two tired\"\n",
      "}\n",
      "{\n",
      "  setup: \"Why couldn't the bicycle stand up by itself?\",\n",
      "  punchline: \"It was two tired.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for await (const s of await chain.stream({ query: jokeQuery })) {\n",
    "    console.log(s)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eefe12b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output) for other techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
